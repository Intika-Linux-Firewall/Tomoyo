Index: security/ccsecurity/domain.c
===================================================================
--- security/ccsecurity/domain.c	(revision 5479)
+++ security/ccsecurity/domain.c	(working copy)
@@ -37,6 +37,8 @@
 	if (mutex_lock_interruptible(&ccs_policy_lock))
 		return -ENOMEM;
 	list_for_each_entry_srcu(entry, list, list, &ccs_ss) {
+		if (entry->is_deleted == CCS_GC_IN_PROGRESS)
+			continue;
 		if (!check_duplicate(entry, new_entry))
 			continue;
 		entry->is_deleted = param->is_delete;
@@ -113,6 +115,8 @@
 	if (mutex_lock_interruptible(&ccs_policy_lock))
 		goto out;
 	list_for_each_entry_srcu(entry, list, list, &ccs_ss) {
+		if (entry->is_deleted == CCS_GC_IN_PROGRESS)
+			continue;
 		if (!ccs_same_acl_head(entry, new_entry) ||
 		    !check_duplicate(entry, new_entry))
 			continue;
@@ -449,7 +453,8 @@
 	if (mutex_lock_interruptible(&ccs_policy_lock))
 		goto out;
 	ptr = ccs_find_namespace(domainname, len);
-	if (!ptr && ccs_memory_ok(entry, sizeof(*entry) + len + 1)) {
+	ccs_set_memory_size(sizeof(*entry) + len + 1);
+	if (!ptr && ccs_memory_ok(entry)) {
 		char *name = (char *) (entry + 1);
 		ptr = entry;
 		memmove(name, domainname, len);
Index: security/ccsecurity/memory.c
===================================================================
--- security/ccsecurity/memory.c	(revision 5479)
+++ security/ccsecurity/memory.c	(working copy)
@@ -29,40 +29,38 @@
 		panic("MAC Initialization failed.\n");
 }
 
-/*
- * Lock for protecting ccs_memory_used.
- *
- * I don't use atomic_t because it can handle only 16MB in 2.4 kernels.
- */
-static DEFINE_SPINLOCK(ccs_policy_memory_lock);
 /* Memoy currently used by policy/audit log/query. */
 unsigned int ccs_memory_used[CCS_MAX_MEMORY_STAT];
 /* Memory quota for "policy"/"audit log"/"query". */
 unsigned int ccs_memory_quota[CCS_MAX_MEMORY_STAT];
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 5, 0)
+/* Protected by ccs_policy_lock mutex. */
+unsigned int ccs_memory_size; /* = ksize(ptr) */
+#endif
 
 /**
  * ccs_memory_ok - Check memory quota.
  *
- * @ptr:  Pointer to allocated memory. Maybe NULL.
- * @size: Size in byte. Not used if @ptr is NULL.
+ * @ptr: Pointer to allocated memory. Maybe NULL.
  *
  * Returns true if @ptr is not NULL and quota not exceeded, false otherwise.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
-bool ccs_memory_ok(const void *ptr, const unsigned int size)
+bool ccs_memory_ok(const void *ptr)
 {
 	if (ptr) {
-		const size_t s = ccs_round2(size);
-		bool result;
-		spin_lock(&ccs_policy_memory_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
+		const size_t s = ksize(ptr);
+#else
+		const size_t s = ccs_memory_size;
+#endif
 		ccs_memory_used[CCS_MEMORY_POLICY] += s;
-		result = !ccs_memory_quota[CCS_MEMORY_POLICY] ||
-			ccs_memory_used[CCS_MEMORY_POLICY] <=
-			ccs_memory_quota[CCS_MEMORY_POLICY];
-		if (!result)
-			ccs_memory_used[CCS_MEMORY_POLICY] -= s;
-		spin_unlock(&ccs_policy_memory_lock);
-		if (result)
+		if (!ccs_memory_quota[CCS_MEMORY_POLICY] ||
+		    ccs_memory_used[CCS_MEMORY_POLICY] <=
+		    ccs_memory_quota[CCS_MEMORY_POLICY])
 			return true;
+		ccs_memory_used[CCS_MEMORY_POLICY] -= s;
 	}
 	ccs_warn_oom(__func__);
 	return false;
@@ -76,11 +74,14 @@
  *
  * Returns pointer to allocated memory on success, NULL otherwise.
  * @data is zero-cleared on success.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
 void *ccs_commit_ok(void *data, const unsigned int size)
 {
 	void *ptr = kmalloc(size, CCS_GFP_FLAGS);
-	if (ccs_memory_ok(ptr, size)) {
+	ccs_set_memory_size(size);
+	if (ccs_memory_ok(ptr)) {
 		memmove(ptr, data, size);
 		memset(data, 0, size);
 		return ptr;
@@ -90,23 +91,6 @@
 }
 
 /**
- * ccs_memory_free - Free memory for elements.
- *
- * @ptr:  Pointer to allocated memory.
- * @size: Size in byte.
- *
- * Returns nothing.
- */
-void ccs_memory_free(const void *ptr, size_t size)
-{
-	size_t s = ccs_round2(size);
-	spin_lock(&ccs_policy_memory_lock);
-	ccs_memory_used[CCS_MEMORY_POLICY] -= s;
-	spin_unlock(&ccs_policy_memory_lock);
-	kfree(ptr);
-}
-
-/**
  * ccs_get_group - Allocate memory for "struct ccs_path_group"/"struct ccs_number_group"/"struct ccs_address_group".
  *
  * @param: Pointer to "struct ccs_acl_param".
@@ -130,7 +114,8 @@
 		goto out;
 	list = &param->ns->group_list[idx];
 	list_for_each_entry(group, list, head.list) {
-		if (e.group_name != group->group_name)
+		if (e.group_name != group->group_name ||
+		    atomic_read(&group->head.users) == CCS_GC_IN_PROGRESS)
 			continue;
 		atomic_inc(&group->head.users);
 		found = true;
@@ -182,14 +167,16 @@
 	if (mutex_lock_interruptible(&ccs_policy_lock))
 		return NULL;
 	list_for_each_entry(ptr, head, head.list) {
-		if (hash != ptr->entry.hash || strcmp(name, ptr->entry.name))
+		if (hash != ptr->entry.hash || strcmp(name, ptr->entry.name) ||
+		    atomic_read(&ptr->head.users) == CCS_GC_IN_PROGRESS)
 			continue;
 		atomic_inc(&ptr->head.users);
 		goto out;
 	}
 	allocated_len = sizeof(*ptr) + len;
 	ptr = kzalloc(allocated_len, CCS_GFP_FLAGS);
-	if (ccs_memory_ok(ptr, allocated_len)) {
+	ccs_set_memory_size(allocated_len);
+	if (ccs_memory_ok(ptr)) {
 		ptr->entry.name = ((char *) ptr) + sizeof(*ptr);
 		memmove((char *) ptr->entry.name, name, len);
 		atomic_set(&ptr->head.users, 1);
Index: security/ccsecurity/file.c
===================================================================
--- security/ccsecurity/file.c	(revision 5479)
+++ security/ccsecurity/file.c	(working copy)
@@ -600,8 +600,8 @@
  *
  * Caller holds ccs_read_lock().
  */
-int ccs_path_permission(struct ccs_request_info *r, u8 operation,
-			const struct ccs_path_info *filename)
+static int ccs_path_permission(struct ccs_request_info *r, u8 operation,
+			       const struct ccs_path_info *filename)
 {
 	int error;
 	r->type = ccs_p2mac[operation];
Index: security/ccsecurity/internal.h
===================================================================
--- security/ccsecurity/internal.h	(revision 5479)
+++ security/ccsecurity/internal.h	(working copy)
@@ -270,6 +270,44 @@
 
 #endif
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+
+/**
+ * __list_del_entry - Deletes entry from list without re-initialization.
+ *
+ * @entry: Pointer to "struct list_head".
+ *
+ * Returns nothing.
+ *
+ * This is for compatibility with older kernels.
+ */
+static inline void __list_del_entry(struct list_head *entry)
+{
+	__list_del(entry->prev, entry->next);
+}
+
+#endif
+
+#ifndef list_for_each_entry_safe
+
+/**
+ * list_for_each_entry_safe - Iterate over list of given type safe against removal of list entry.
+ *
+ * @pos:    The "type *" to use as a loop cursor.
+ * @n:      Another "type *" to use as temporary storage.
+ * @head:   Pointer to "struct list_head".
+ * @member: The name of the list_struct within the struct.
+ *
+ * This is for compatibility with older kernels.
+ */
+#define list_for_each_entry_safe(pos, n, head, member)                  \
+	for (pos = list_entry((head)->next, typeof(*pos), member),      \
+		     n = list_entry(pos->member.next, typeof(*pos), member); \
+	     &pos->member != (head);					\
+	     pos = n, n = list_entry(n->member.next, typeof(*n), member))
+
+#endif
+
 #ifndef srcu_dereference
 
 /**
@@ -717,6 +755,9 @@
 /* Size of temporary buffer for execve() operation. */
 #define CCS_EXEC_TMPSIZE     4096
 
+/* Garbage collector is trying to kfree() this element. */
+#define CCS_GC_IN_PROGRESS -1
+
 /* Profile number is an integer between 0 and 255. */
 #define CCS_MAX_PROFILES 256
 
@@ -769,7 +810,7 @@
 /* Common header for holding ACL entries. */
 struct ccs_acl_head {
 	struct list_head list;
-	bool is_deleted;
+	s8 is_deleted; /* true or false or CCS_GC_IN_PROGRESS */
 } __attribute__((__packed__));
 
 /* Common header for shared entries. */
@@ -782,7 +823,7 @@
 struct ccs_acl_info {
 	struct list_head list;
 	struct ccs_condition *cond; /* Maybe NULL. */
-	bool is_deleted;
+	s8 is_deleted; /* true or false or CCS_GC_IN_PROGRESS */
 	u8 type; /* One of values in "enum ccs_acl_entry_type_index". */
 } __attribute__((__packed__));
 
@@ -1380,7 +1421,7 @@
 bool ccs_domain_quota_ok(struct ccs_request_info *r);
 bool ccs_dump_page(struct linux_binprm *bprm, unsigned long pos,
 		   struct ccs_page_dump *dump);
-bool ccs_memory_ok(const void *ptr, const unsigned int size);
+bool ccs_memory_ok(const void *ptr);
 bool ccs_number_matches_group(const unsigned long min, const unsigned long max,
 			      const struct ccs_group *group);
 bool ccs_parse_ipaddr_union(struct ccs_acl_param *param,
@@ -1415,8 +1456,6 @@
 int ccs_init_request_info(struct ccs_request_info *r, const u8 index);
 int ccs_open_control(const u8 type, struct file *file);
 int ccs_parse_ip_address(struct ccs_acl_param *param, u16 *min, u16 *max);
-int ccs_path_permission(struct ccs_request_info *r, u8 operation,
-			const struct ccs_path_info *filename);
 int ccs_poll_control(struct file *file, poll_table *wait);
 int ccs_poll_log(struct file *file, poll_table *wait);
 int ccs_supervisor(struct ccs_request_info *r, const char *fmt, ...)
@@ -1465,7 +1504,6 @@
 void ccs_fill_path_info(struct ccs_path_info *ptr);
 void ccs_get_attributes(struct ccs_obj_info *obj);
 void ccs_init_policy_namespace(struct ccs_policy_namespace *ns);
-void ccs_memory_free(const void *ptr, size_t size);
 void ccs_normalize_line(unsigned char *buffer);
 void ccs_notify_gc(struct ccs_io_buffer *head, const bool is_register);
 void ccs_print_ip(char *buf, const unsigned int size,
@@ -1828,7 +1866,39 @@
 
 #endif
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
 /**
+ * ccs_set_memory_size - Set memory size to be allocated.
+ *
+ * @size: Unused.
+ *
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
+ */
+static inline void ccs_set_memory_size(const unsigned int size)
+{
+	/* ccs_memory_ok() uses ksize(). */
+}
+#else
+extern unsigned int ccs_memory_size;
+/**
+ * ccs_set_memory_size - Set memory size to be allocated.
+ *
+ * @size: Memory size in byte.
+ *
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
+ */
+static inline void ccs_set_memory_size(const unsigned int size)
+{
+	/* ccs_memory_ok() uses ccs_round2(size). */
+	ccs_memory_size = size;
+}
+#endif
+
+/**
  * ccs_put_condition - Drop reference on "struct ccs_condition".
  *
  * @cond: Pointer to "struct ccs_condition". Maybe NULL.
Index: security/ccsecurity/condition.c
===================================================================
--- security/ccsecurity/condition.c	(revision 5479)
+++ security/ccsecurity/condition.c	(working copy)
@@ -407,9 +407,9 @@
 		found = true;
 		goto out;
 	}
-	list_for_each_entry_srcu(ptr, &ccs_condition_list, head.list,
-				 &ccs_ss) {
-		if (!ccs_same_condition(ptr, entry))
+	list_for_each_entry(ptr, &ccs_condition_list, head.list) {
+		if (!ccs_same_condition(ptr, entry) ||
+		    atomic_read(&ptr->head.users) == CCS_GC_IN_PROGRESS)
 			continue;
 		/* Same entry found. Share this entry. */
 		atomic_inc(&ptr->head.users);
@@ -417,9 +417,10 @@
 		break;
 	}
 	if (!found) {
-		if (ccs_memory_ok(entry, entry->size)) {
+		ccs_set_memory_size(entry->size);
+		if (ccs_memory_ok(entry)) {
 			atomic_set(&entry->head.users, 1);
-			list_add_rcu(&entry->head.list, &ccs_condition_list);
+			list_add(&entry->head.list, &ccs_condition_list);
 		} else {
 			found = true;
 			ptr = NULL;
Index: security/ccsecurity/policy_io.c
===================================================================
--- security/ccsecurity/policy_io.c	(revision 5479)
+++ security/ccsecurity/policy_io.c	(working copy)
@@ -498,7 +498,8 @@
 	if (mutex_lock_interruptible(&ccs_policy_lock))
 		goto out;
 	ptr = ns->profile_ptr[profile];
-	if (!ptr && ccs_memory_ok(entry, sizeof(*entry))) {
+	ccs_set_memory_size(sizeof(*entry));
+	if (!ptr && ccs_memory_ok(entry)) {
 		ptr = entry;
 		ptr->default_config = CCS_CONFIG_DISABLED |
 			CCS_CONFIG_WANT_GRANT_LOG | CCS_CONFIG_WANT_REJECT_LOG;
Index: security/ccsecurity/gc.c
===================================================================
--- security/ccsecurity/gc.c	(revision 5479)
+++ security/ccsecurity/gc.c	(working copy)
@@ -8,83 +8,88 @@
 
 #include "internal.h"
 
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 5, 0)
-
-#ifndef LIST_POISON2
-#define LIST_POISON2  ((void *) 0x00200200)
-#endif
-
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
 /**
- * list_del_rcu - Deletes entry from list without re-initialization.
+ * ccs_memory_free - Free memory for elements.
  *
- * @entry: Pointer to "struct list_head".
+ * @ptr: Pointer to allocated memory.
  *
  * Returns nothing.
  *
- * This is for compatibility with older kernels.
+ * Caller holds ccs_policy_lock mutex.
  */
-static inline void list_del_rcu(struct list_head *entry)
+static inline void ccs_memory_free(const void *ptr)
 {
-	__list_del(entry->prev, entry->next);
-	entry->prev = LIST_POISON2;
+	ccs_memory_used[CCS_MEMORY_POLICY] -= ksize(ptr);
+	kfree(ptr);
 }
-
-#endif
-
-#ifndef list_for_each_entry_safe
-
+#else
 /**
- * list_for_each_entry_safe - Iterate over list of given type safe against removal of list entry.
+ * ccs_memory_free - Free memory for elements.
  *
- * @pos:    The "type *" to use as a loop cursor.
- * @n:      Another "type *" to use as temporary storage.
- * @head:   Pointer to "struct list_head".
- * @member: The name of the list_struct within the struct.
+ * @ptr:  Pointer to allocated memory.
+ * @type: One of values in "enum ccs_policy_id".
  *
- * This is for compatibility with older kernels.
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
-#define list_for_each_entry_safe(pos, n, head, member)                  \
-	for (pos = list_entry((head)->next, typeof(*pos), member),      \
-		     n = list_entry(pos->member.next, typeof(*pos), member); \
-	     &pos->member != (head);					\
-	     pos = n, n = list_entry(n->member.next, typeof(*n), member))
-
+static inline void ccs_memory_free(const void *ptr,
+				   const enum ccs_policy_id type)
+{
+	/* Size of an element. */
+	static const u8 element[CCS_MAX_POLICY] = {
+		[CCS_ID_RESERVEDPORT] = sizeof(struct ccs_reserved),
+		[CCS_ID_GROUP] = sizeof(struct ccs_group),
+		[CCS_ID_ADDRESS_GROUP] = sizeof(struct ccs_address_group),
+		[CCS_ID_PATH_GROUP] = sizeof(struct ccs_path_group),
+		[CCS_ID_NUMBER_GROUP] = sizeof(struct ccs_number_group),
+		[CCS_ID_AGGREGATOR] = sizeof(struct ccs_aggregator),
+		[CCS_ID_TRANSITION_CONTROL]
+		= sizeof(struct ccs_transition_control),
+		[CCS_ID_MANAGER] = sizeof(struct ccs_manager),
+		/* [CCS_ID_CONDITION] = "struct ccs_condition"->size, */
+		/* [CCS_ID_NAME] = "struct ccs_name"->size, */
+		/* [CCS_ID_ACL] = acl["struct ccs_acl_info"->type], */
+		[CCS_ID_DOMAIN] = sizeof(struct ccs_domain_info),
+	};
+	/* Size of a domain ACL element. */
+	static const u8 acl[] = {
+		[CCS_TYPE_PATH_ACL] = sizeof(struct ccs_path_acl),
+		[CCS_TYPE_PATH2_ACL] = sizeof(struct ccs_path2_acl),
+		[CCS_TYPE_PATH_NUMBER_ACL]
+		= sizeof(struct ccs_path_number_acl),
+		[CCS_TYPE_MKDEV_ACL] = sizeof(struct ccs_mkdev_acl),
+		[CCS_TYPE_MOUNT_ACL] = sizeof(struct ccs_mount_acl),
+		[CCS_TYPE_INET_ACL] = sizeof(struct ccs_inet_acl),
+		[CCS_TYPE_UNIX_ACL] = sizeof(struct ccs_unix_acl),
+		[CCS_TYPE_ENV_ACL] = sizeof(struct ccs_env_acl),
+		[CCS_TYPE_CAPABILITY_ACL] = sizeof(struct ccs_capability_acl),
+		[CCS_TYPE_SIGNAL_ACL] = sizeof(struct ccs_signal_acl),
+		[CCS_TYPE_AUTO_EXECUTE_HANDLER]
+		= sizeof(struct ccs_handler_acl),
+		[CCS_TYPE_DENIED_EXECUTE_HANDLER]
+		= sizeof(struct ccs_handler_acl),
+		[CCS_TYPE_AUTO_TASK_ACL] = sizeof(struct ccs_task_acl),
+		[CCS_TYPE_MANUAL_TASK_ACL] = sizeof(struct ccs_task_acl),
+	};
+	size_t size;
+	if (type == CCS_ID_ACL)
+		size = acl[container_of(ptr, typeof(struct ccs_acl_info),
+					list)->type];
+	else if (type == CCS_ID_NAME)
+		size = container_of(ptr, typeof(struct ccs_name),
+				    head.list)->size;
+	else if (type == CCS_ID_CONDITION)
+		size = container_of(ptr, typeof(struct ccs_condition),
+				    head.list)->size;
+	else
+		size = element[type];
+	ccs_memory_used[CCS_MEMORY_POLICY] -= ccs_round2(size);
+	kfree(ptr);
+}
 #endif
 
-/* Size of an element. */
-static const u8 ccs_element_size[CCS_MAX_POLICY] = {
-	[CCS_ID_RESERVEDPORT] = sizeof(struct ccs_reserved),
-	[CCS_ID_GROUP] = sizeof(struct ccs_group),
-	[CCS_ID_ADDRESS_GROUP] = sizeof(struct ccs_address_group),
-	[CCS_ID_PATH_GROUP] = sizeof(struct ccs_path_group),
-	[CCS_ID_NUMBER_GROUP] = sizeof(struct ccs_number_group),
-	[CCS_ID_AGGREGATOR] = sizeof(struct ccs_aggregator),
-	[CCS_ID_TRANSITION_CONTROL] = sizeof(struct ccs_transition_control),
-	[CCS_ID_MANAGER] = sizeof(struct ccs_manager),
-	/* [CCS_ID_CONDITION] = "struct ccs_condition"->size, */
-	/* [CCS_ID_NAME] = "struct ccs_name"->size, */
-	/* [CCS_ID_ACL] = ccs_acl_size["struct ccs_acl_info"->type], */
-	[CCS_ID_DOMAIN] = sizeof(struct ccs_domain_info),
-};
-
-/* Size of a domain ACL element. */
-static const u8 ccs_acl_size[] = {
-	[CCS_TYPE_PATH_ACL] = sizeof(struct ccs_path_acl),
-	[CCS_TYPE_PATH2_ACL] = sizeof(struct ccs_path2_acl),
-	[CCS_TYPE_PATH_NUMBER_ACL] = sizeof(struct ccs_path_number_acl),
-	[CCS_TYPE_MKDEV_ACL] = sizeof(struct ccs_mkdev_acl),
-	[CCS_TYPE_MOUNT_ACL] = sizeof(struct ccs_mount_acl),
-	[CCS_TYPE_INET_ACL] = sizeof(struct ccs_inet_acl),
-	[CCS_TYPE_UNIX_ACL] = sizeof(struct ccs_unix_acl),
-	[CCS_TYPE_ENV_ACL] = sizeof(struct ccs_env_acl),
-	[CCS_TYPE_CAPABILITY_ACL] = sizeof(struct ccs_capability_acl),
-	[CCS_TYPE_SIGNAL_ACL] = sizeof(struct ccs_signal_acl),
-	[CCS_TYPE_AUTO_EXECUTE_HANDLER] = sizeof(struct ccs_handler_acl),
-	[CCS_TYPE_DENIED_EXECUTE_HANDLER] = sizeof(struct ccs_handler_acl),
-	[CCS_TYPE_AUTO_TASK_ACL] = sizeof(struct ccs_task_acl),
-	[CCS_TYPE_MANUAL_TASK_ACL] = sizeof(struct ccs_task_acl),
-};
-
 /* The list for "struct ccs_io_buffer". */
 static LIST_HEAD(ccs_io_buffer_list);
 /* Lock for protecting ccs_io_buffer_list. */
@@ -105,15 +110,11 @@
 	list_for_each_entry(head, &ccs_io_buffer_list, list) {
 		head->users++;
 		spin_unlock(&ccs_io_buffer_list_lock);
-		if (mutex_lock_interruptible(&head->io_sem)) {
-			in_use = true;
-			goto out;
-		}
+		mutex_lock(&head->io_sem);
 		if (head->r.domain == element || head->r.group == element ||
 		    head->r.acl == element || &head->w.domain->list == element)
 			in_use = true;
 		mutex_unlock(&head->io_sem);
-out:
 		spin_lock(&ccs_io_buffer_list_lock);
 		head->users--;
 		if (in_use)
@@ -127,23 +128,20 @@
  * ccs_name_used_by_io_buffer - Check whether the string is used by /proc/ccs/ users or not.
  *
  * @string: String to check.
- * @size:   Memory allocated for @string .
  *
  * Returns true if @string is used by /proc/ccs/ users, false otherwise.
  */
-static bool ccs_name_used_by_io_buffer(const char *string, const size_t size)
+static bool ccs_name_used_by_io_buffer(const char *string)
 {
 	struct ccs_io_buffer *head;
 	bool in_use = false;
+	const size_t size = strlen(string) + 1;
 	spin_lock(&ccs_io_buffer_list_lock);
 	list_for_each_entry(head, &ccs_io_buffer_list, list) {
 		int i;
 		head->users++;
 		spin_unlock(&ccs_io_buffer_list_lock);
-		if (mutex_lock_interruptible(&head->io_sem)) {
-			in_use = true;
-			goto out;
-		}
+		mutex_lock(&head->io_sem);
 		for (i = 0; i < CCS_MAX_IO_READ_QUEUE; i++) {
 			const char *w = head->r.w[i];
 			if (w < string || w > string + size)
@@ -152,7 +150,6 @@
 			break;
 		}
 		mutex_unlock(&head->io_sem);
-out:
 		spin_lock(&ccs_io_buffer_list_lock);
 		head->users--;
 		if (in_use)
@@ -162,86 +159,7 @@
 	return in_use;
 }
 
-/* Structure for garbage collection. */
-struct ccs_gc {
-	struct list_head list;
-	enum ccs_policy_id type;
-	size_t size;
-	struct list_head *element;
-};
-/* List of entries to be deleted. */
-static LIST_HEAD(ccs_gc_list);
-/* Length of ccs_gc_list. */
-static int ccs_gc_list_len;
-
 /**
- * ccs_add_to_gc - Add an entry to to be deleted list.
- *
- * @type:    One of values in "enum ccs_policy_id".
- * @element: Pointer to "struct list_head".
- *
- * Returns true on success, false otherwise.
- *
- * Caller holds ccs_policy_lock mutex.
- *
- * Adding an entry needs kmalloc(). Thus, if we try to add thousands of
- * entries at once, it will take too long time. Thus, do not add more than 128
- * entries per a scan. But to be able to handle worst case where all entries
- * are in-use, we accept one more entry per a scan.
- *
- * If we use singly linked list using "struct list_head"->prev (which is
- * LIST_POISON2), we can avoid kmalloc().
- */
-static bool ccs_add_to_gc(const enum ccs_policy_id type,
-			  struct list_head *element)
-{
-	struct ccs_gc *entry = kzalloc(sizeof(*entry), CCS_GFP_FLAGS);
-	if (!entry)
-		return false;
-	entry->type = type;
-	if (type == CCS_ID_ACL)
-		entry->size =
-			ccs_acl_size[container_of(element,
-						  typeof(struct ccs_acl_info),
-						  list)->type];
-	else if (type == CCS_ID_NAME)
-		entry->size =
-			container_of(element, typeof(struct ccs_name),
-				     head.list)->size;
-	else if (type == CCS_ID_CONDITION)
-		entry->size =
-			container_of(element, typeof(struct ccs_condition),
-				     head.list)->size;
-	else
-		entry->size = ccs_element_size[type];
-	entry->element = element;
-	list_add(&entry->list, &ccs_gc_list);
-	list_del_rcu(element);
-	return ccs_gc_list_len++ < 128;
-}
-
-/**
- * ccs_element_linked_by_gc - Validate next element of an entry.
- *
- * @element: Pointer to an element.
- * @size:    Size of @element in byte.
- *
- * Returns true if @element is linked by other elements in the garbage
- * collector's queue, false otherwise.
- */
-static bool ccs_element_linked_by_gc(const u8 *element, const size_t size)
-{
-	struct ccs_gc *p;
-	list_for_each_entry(p, &ccs_gc_list, list) {
-		const u8 *ptr = (const u8 *) p->element->next;
-		if (ptr < element || element + size < ptr)
-			continue;
-		return true;
-	}
-	return false;
-}
-
-/**
  * ccs_del_transition_control - Delete members in "struct ccs_transition_control".
  *
  * @element: Pointer to "struct list_head".
@@ -471,6 +389,8 @@
  * @element: Pointer to "struct list_head".
  *
  * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
 static inline void ccs_del_domain(struct list_head *element)
 {
@@ -479,13 +399,17 @@
 	struct ccs_acl_info *acl;
 	struct ccs_acl_info *tmp;
 	/*
-	 * Since this domain is referenced from none of "struct ccs_io_buffer"
-	 * ccs_gc_list, "struct task_struct", we can delete elements without
-	 * checking for is_deleted flag.
+	 * Since this domain is referenced from neither "struct ccs_io_buffer"
+	 * nor "struct task_struct", we can delete elements without checking
+	 * for is_deleted flag.
 	 */
 	list_for_each_entry_safe(acl, tmp, &domain->acl_info_list, list) {
 		ccs_del_acl(&acl->list);
-		ccs_memory_free(acl, ccs_acl_size[acl->type]);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
+		ccs_memory_free(acl);
+#else
+		ccs_memory_free(acl, CCS_ID_ACL);
+#endif
 	}
 	ccs_put_name(domain->domainname);
 }
@@ -692,24 +616,138 @@
 #endif
 
 /**
+ * ccs_try_to_gc - Try to kfree() an entry.
+ *
+ * @type:    One of values in "enum ccs_policy_id".
+ * @element: Pointer to "struct list_head".
+ *
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
+ */
+static void ccs_try_to_gc(const enum ccs_policy_id type,
+			  struct list_head *element)
+{
+	/*
+	 * __list_del_entry() guarantees that the list element became no longer
+	 * reachable from the list which the element was originally on (e.g.
+	 * ccs_domain_list). Also, synchronize_srcu() guarantees that the list
+	 * element became no longer referenced by syscall users.
+	 */
+	__list_del_entry(element);
+	mutex_unlock(&ccs_policy_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
+	synchronize_srcu(&ccs_ss);
+#else
+	ccs_synchronize_counter();
+#endif
+	/*
+	 * However, there are two users which may still be using the list
+	 * element. We need to defer until both users forget this element.
+	 *
+	 * Don't kfree() until "struct ccs_io_buffer"->r.{domain,group,acl} and
+	 * "struct ccs_io_buffer"->w.domain forget this element.
+	 */
+	if (ccs_struct_used_by_io_buffer(element))
+		goto reinject;
+	switch (type) {
+	case CCS_ID_TRANSITION_CONTROL:
+		ccs_del_transition_control(element);
+		break;
+	case CCS_ID_MANAGER:
+		ccs_del_manager(element);
+		break;
+	case CCS_ID_AGGREGATOR:
+		ccs_del_aggregator(element);
+		break;
+	case CCS_ID_GROUP:
+		ccs_del_group(element);
+		break;
+	case CCS_ID_PATH_GROUP:
+		ccs_del_path_group(element);
+		break;
+	case CCS_ID_ADDRESS_GROUP:
+		ccs_del_address_group(element);
+		break;
+	case CCS_ID_NUMBER_GROUP:
+		ccs_del_number_group(element);
+		break;
+	case CCS_ID_RESERVEDPORT:
+		ccs_del_reservedport(element);
+		break;
+	case CCS_ID_CONDITION:
+		ccs_del_condition(element);
+		break;
+	case CCS_ID_NAME:
+		/*
+		 * Don't kfree() until all "struct ccs_io_buffer"->r.w[] forget
+		 * this element.
+		 */
+		if (ccs_name_used_by_io_buffer
+		    (container_of(element, typeof(struct ccs_name),
+				  head.list)->entry.name))
+			goto reinject;
+		ccs_del_name(element);
+		break;
+	case CCS_ID_ACL:
+		ccs_del_acl(element);
+		break;
+	case CCS_ID_DOMAIN:
+		/*
+		 * Don't kfree() until all "struct task_struct" forget this
+		 * element.
+		 */
+		if (ccs_domain_used_by_task
+		    (container_of(element, typeof(struct ccs_domain_info),
+				  list)))
+			goto reinject;
+		break;
+	case CCS_MAX_POLICY:
+		break;
+	}
+	mutex_lock(&ccs_policy_lock);
+	if (type == CCS_ID_DOMAIN)
+		ccs_del_domain(element);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
+	ccs_memory_free(element);
+#else
+	ccs_memory_free(element, type);
+#endif
+	return;
+reinject:
+	/*
+	 * We can safely reinject this element here bacause
+	 * (1) Appending list elements and removing list elements are protected
+	 *     by ccs_policy_lock mutex.
+	 * (2) Only this function removes list elements and this function is
+	 *     exclusively executed by ccs_gc_mutex mutex.
+	 * are true.
+	 */
+	mutex_lock(&ccs_policy_lock);
+	list_add_rcu(element, element->prev);
+}
+
+/**
  * ccs_collect_member - Delete elements with "struct ccs_acl_head".
  *
  * @id:          One of values in "enum ccs_policy_id".
  * @member_list: Pointer to "struct list_head".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
-static bool ccs_collect_member(const enum ccs_policy_id id,
+static void ccs_collect_member(const enum ccs_policy_id id,
 			       struct list_head *member_list)
 {
 	struct ccs_acl_head *member;
-	list_for_each_entry(member, member_list, list) {
+	struct ccs_acl_head *tmp;
+	list_for_each_entry_safe(member, tmp, member_list, list) {
 		if (!member->is_deleted)
 			continue;
-		if (!ccs_add_to_gc(id, &member->list))
-			return false;
+		member->is_deleted = CCS_GC_IN_PROGRESS;
+		ccs_try_to_gc(id, &member->list);
 	}
-	return true;
 }
 
 /**
@@ -717,22 +755,24 @@
  *
  * @list: Pointer to "struct list_head".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
  */
-static bool ccs_collect_acl(struct list_head *list)
+static void ccs_collect_acl(struct list_head *list)
 {
 	struct ccs_acl_info *acl;
-	list_for_each_entry(acl, list, list) {
+	struct ccs_acl_info *tmp;
+	list_for_each_entry_safe(acl, tmp, list, list) {
 		if (!acl->is_deleted)
 			continue;
-		if (!ccs_add_to_gc(CCS_ID_ACL, &acl->list))
-			return false;
+		acl->is_deleted = CCS_GC_IN_PROGRESS;
+		ccs_try_to_gc(CCS_ID_ACL, &acl->list);
 	}
-	return true;
 }
 
 /**
- * ccs_collect_entry - Scan lists for deleted elements.
+ * ccs_collect_entry - Try to kfree() deleted elements.
  *
  * Returns nothing.
  */
@@ -741,33 +781,39 @@
 	int i;
 	enum ccs_policy_id id;
 	struct ccs_policy_namespace *ns;
-	int idx;
-	if (mutex_lock_interruptible(&ccs_policy_lock))
-		return;
-	idx = ccs_read_lock();
+	mutex_lock(&ccs_policy_lock);
 	{
 		struct ccs_domain_info *domain;
-		list_for_each_entry(domain, &ccs_domain_list, list) {
-			if (!ccs_collect_acl(&domain->acl_info_list))
-				goto unlock;
+		struct ccs_domain_info *tmp;
+		list_for_each_entry_safe(domain, tmp, &ccs_domain_list, list) {
+			ccs_collect_acl(&domain->acl_info_list);
 			if (!domain->is_deleted ||
 			    ccs_domain_used_by_task(domain))
 				continue;
-			if (!ccs_add_to_gc(CCS_ID_DOMAIN, &domain->list))
-				goto unlock;
+			ccs_try_to_gc(CCS_ID_DOMAIN, &domain->list);
 		}
 	}
-	list_for_each_entry_srcu(ns, &ccs_namespace_list, namespace_list,
-				 &ccs_ss) {
+	list_for_each_entry(ns, &ccs_namespace_list, namespace_list) {
 		for (id = 0; id < CCS_MAX_POLICY; id++)
-			if (!ccs_collect_member(id, &ns->policy_list[id]))
-				goto unlock;
+			ccs_collect_member(id, &ns->policy_list[id]);
 		for (i = 0; i < CCS_MAX_ACL_GROUPS; i++)
-			if (!ccs_collect_acl(&ns->acl_group[i]))
-				goto unlock;
+			ccs_collect_acl(&ns->acl_group[i]);
+	}
+	{
+		struct ccs_shared_acl_head *ptr;
+		struct ccs_shared_acl_head *tmp;
+		list_for_each_entry_safe(ptr, tmp, &ccs_condition_list, list) {
+			if (atomic_read(&ptr->users) > 0)
+				continue;
+			atomic_set(&ptr->users, CCS_GC_IN_PROGRESS);
+			ccs_try_to_gc(CCS_ID_CONDITION, &ptr->list);
+		}
+	}
+	list_for_each_entry(ns, &ccs_namespace_list, namespace_list) {
 		for (i = 0; i < CCS_MAX_GROUP; i++) {
 			struct list_head *list = &ns->group_list[i];
 			struct ccs_group *group;
+			struct ccs_group *tmp;
 			switch (i) {
 			case 0:
 				id = CCS_ID_PATH_GROUP;
@@ -779,154 +825,48 @@
 				id = CCS_ID_ADDRESS_GROUP;
 				break;
 			}
-			list_for_each_entry(group, list, head.list) {
-				if (!ccs_collect_member(id,
-							&group->member_list))
-					goto unlock;
+			list_for_each_entry_safe(group, tmp, list, head.list) {
+				ccs_collect_member(id, &group->member_list);
 				if (!list_empty(&group->member_list) ||
-				    atomic_read(&group->head.users))
+				    atomic_read(&group->head.users) > 0)
 					continue;
-				if (!ccs_add_to_gc(CCS_ID_GROUP,
-						   &group->head.list))
-					goto unlock;
+				atomic_set(&group->head.users,
+					   CCS_GC_IN_PROGRESS);
+				ccs_try_to_gc(CCS_ID_GROUP, &group->head.list);
 			}
 		}
 	}
-	id = CCS_ID_CONDITION;
-	for (i = 0; i < CCS_MAX_HASH + 1; i++) {
-		struct list_head *list = !i ?
-			&ccs_condition_list : &ccs_name_list[i - 1];
+	for (i = 0; i < CCS_MAX_HASH; i++) {
+		struct list_head *list = &ccs_name_list[i];
 		struct ccs_shared_acl_head *ptr;
-		list_for_each_entry(ptr, list, list) {
-			if (atomic_read(&ptr->users))
+		struct ccs_shared_acl_head *tmp;
+		list_for_each_entry_safe(ptr, tmp, list, list) {
+			if (atomic_read(&ptr->users) > 0)
 				continue;
-			if (!ccs_add_to_gc(id, &ptr->list))
-				goto unlock;
+			atomic_set(&ptr->users, CCS_GC_IN_PROGRESS);
+			ccs_try_to_gc(CCS_ID_NAME, &ptr->list);
 		}
-		id = CCS_ID_NAME;
 	}
-unlock:
-	ccs_read_unlock(idx);
 	mutex_unlock(&ccs_policy_lock);
 }
 
 /**
- * ccs_kfree_entry - Delete entries in ccs_gc_list.
- *
- * Returns true if some entries were kfree()d, false otherwise.
- */
-static bool ccs_kfree_entry(void)
-{
-	struct ccs_gc *p;
-	struct ccs_gc *tmp;
-	bool result = false;
-	list_for_each_entry_safe(p, tmp, &ccs_gc_list, list) {
-		struct list_head * const element = p->element;
-		/*
-		 * list_del_rcu() in ccs_add_to_gc() guarantees that the list
-		 * element became no longer reachable from the list which the
-		 * element was originally on (e.g. ccs_domain_list). Also,
-		 * synchronize_srcu() in ccs_gc_thread() guarantees that the
-		 * list element became no longer referenced by syscall users.
-		 *
-		 * However, there are three users which may still be using the
-		 * list element. We need to defer until all of these users
-		 * forget the list element.
-		 *
-		 * Firstly, defer until "struct ccs_io_buffer"->r.{domain,
-		 * group,acl} and "struct ccs_io_buffer"->w.domain forget the
-		 * list element.
-		 */
-		if (ccs_struct_used_by_io_buffer(element))
-			continue;
-		/*
-		 * Secondly, defer until all other elements in the ccs_gc_list
-		 * list forget the list element.
-		 */
-		if (ccs_element_linked_by_gc((const u8 *) element, p->size))
-			continue;
-		switch (p->type) {
-		case CCS_ID_TRANSITION_CONTROL:
-			ccs_del_transition_control(element);
-			break;
-		case CCS_ID_MANAGER:
-			ccs_del_manager(element);
-			break;
-		case CCS_ID_AGGREGATOR:
-			ccs_del_aggregator(element);
-			break;
-		case CCS_ID_GROUP:
-			ccs_del_group(element);
-			break;
-		case CCS_ID_PATH_GROUP:
-			ccs_del_path_group(element);
-			break;
-		case CCS_ID_ADDRESS_GROUP:
-			ccs_del_address_group(element);
-			break;
-		case CCS_ID_NUMBER_GROUP:
-			ccs_del_number_group(element);
-			break;
-		case CCS_ID_RESERVEDPORT:
-			ccs_del_reservedport(element);
-			break;
-		case CCS_ID_CONDITION:
-			ccs_del_condition(element);
-			break;
-		case CCS_ID_NAME:
-			/*
-			 * Thirdly, defer until all "struct ccs_io_buffer"
-			 * ->r.w[] forget the list element.
-			 */
-			if (ccs_name_used_by_io_buffer(
-			    container_of(element, typeof(struct ccs_name),
-					 head.list)->entry.name, p->size))
-				continue;
-			ccs_del_name(element);
-			break;
-		case CCS_ID_ACL:
-			ccs_del_acl(element);
-			break;
-		case CCS_ID_DOMAIN:
-			/*
-			 * Thirdly, defer until all "struct task_struct" forget
-			 * the list element.
-			 */
-			if (ccs_domain_used_by_task(
-			    container_of(element,
-					 typeof(struct ccs_domain_info),
-					 list)))
-				continue;
-			ccs_del_domain(element);
-			break;
-		case CCS_MAX_POLICY:
-			break;
-		}
-		ccs_memory_free(element, p->size);
-		list_del(&p->list);
-		kfree(p);
-		ccs_gc_list_len--;
-		result = true;
-	}
-	return result;
-}
-
-/**
  * ccs_gc_thread - Garbage collector thread function.
  *
  * @unused: Unused.
  *
- * In case OOM-killer choose this thread for termination, we create this thread
- * as a short live thread whenever /proc/ccs/ interface was close()d.
- *
  * Returns 0.
  */
 static int ccs_gc_thread(void *unused)
 {
 	/* Garbage collector thread is exclusive. */
 	static DEFINE_MUTEX(ccs_gc_mutex);
-	if (!mutex_trylock(&ccs_gc_mutex))
-		goto out;
+	static atomic_t ccs_gc_retry = ATOMIC_INIT(0);
+	if (!mutex_trylock(&ccs_gc_mutex)) {
+		atomic_set(&ccs_gc_retry, 1);
+		/* This acts as do_exit(0). */
+		return 0;
+	}
 #if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 6)
 	/* daemonize() not needed. */
 #elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
@@ -954,15 +894,9 @@
 	snprintf(current->comm, sizeof(current->comm) - 1, "GC for CCS");
 #endif
 	do {
+		atomic_set(&ccs_gc_retry, 0);
 		ccs_collect_entry();
-		if (list_empty(&ccs_gc_list))
-			break;
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
-		synchronize_srcu(&ccs_ss);
-#else
-		ccs_synchronize_counter();
-#endif
-	} while (ccs_kfree_entry());
+	} while (atomic_read(&ccs_gc_retry));
 	{
 		struct ccs_io_buffer *head;
 		struct ccs_io_buffer *tmp;
@@ -979,7 +913,6 @@
 		spin_unlock(&ccs_io_buffer_list_lock);
 	}
 	mutex_unlock(&ccs_gc_mutex);
-out:
 	/* This acts as do_exit(0). */
 	return 0;
 }
