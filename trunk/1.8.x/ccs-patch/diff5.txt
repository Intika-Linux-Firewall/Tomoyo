Index: security/ccsecurity/internal.h
===================================================================
--- security/ccsecurity/internal.h	(revision 4660)
+++ security/ccsecurity/internal.h	(working copy)
@@ -1314,15 +1314,13 @@
 	bool is_delete;
 };
 
-/* Structure for reading/writing policy via /proc interfaces. */
+/* Structure for reading/writing policy via /proc/ccs/ interfaces. */
 struct ccs_io_buffer {
 	void (*read) (struct ccs_io_buffer *);
 	int (*write) (struct ccs_io_buffer *);
 	int (*poll) (struct file *file, poll_table *wait);
 	/* Exclusive lock for this structure.   */
 	struct mutex io_sem;
-	/* Index returned by ccs_lock().        */
-	int reader_idx;
 	char __user *read_user_buf;
 	int read_user_buf_avail;
 	struct {
@@ -1358,6 +1356,10 @@
 	int writebuf_size;
 	/* One of values in "enum ccs_proc_interface_index". */
 	u8 type;
+	/* Users counter protected by ccs_io_buffer_list_lock. */
+	u8 users;
+	/* List for telling GC not to kfree() elements. */
+	struct list_head list;
 };
 
 /* Structure for /proc/ccs/profile interface. */
@@ -1434,7 +1436,6 @@
 int ccs_env_perm(struct ccs_request_info *r, const char *env);
 int ccs_get_path(const char *pathname, struct path *path);
 int ccs_init_request_info(struct ccs_request_info *r, const u8 index);
-int ccs_lock(void);
 int ccs_open_control(const u8 type, struct file *file);
 int ccs_parse_ip_address(char *address, u16 *min, u16 *max);
 int ccs_path_permission(struct ccs_request_info *r, u8 operation,
@@ -1489,6 +1490,7 @@
 void ccs_get_attributes(struct ccs_obj_info *obj);
 void ccs_memory_free(const void *ptr, size_t size);
 void ccs_normalize_line(unsigned char *buffer);
+void ccs_notify_gc(struct ccs_io_buffer *head, const bool is_register);
 void ccs_print_ipv4(char *buffer, const int buffer_len, const u32 min_ip,
 		    const u32 max_ip);
 void ccs_print_ipv6(char *buffer, const int buffer_len,
@@ -1499,9 +1501,7 @@
 void ccs_put_name_union(struct ccs_name_union *ptr);
 void ccs_put_number_union(struct ccs_number_union *ptr);
 void ccs_read_log(struct ccs_io_buffer *head);
-void ccs_run_gc(void);
 void ccs_transition_failed(const char *domainname);
-void ccs_unlock(const int idx);
 void ccs_update_stat(const u8 index);
 void ccs_warn_oom(const char *function);
 void ccs_write_log(struct ccs_request_info *r, const char *fmt, ...)
@@ -1613,6 +1613,9 @@
 
 #else
 
+int ccs_lock(void);
+void ccs_unlock(const int idx);
+
 /**
  * ccs_read_lock - Take lock for protecting policy.
  *
Index: security/ccsecurity/policy_io.c
===================================================================
--- security/ccsecurity/policy_io.c	(revision 4660)
+++ security/ccsecurity/policy_io.c	(working copy)
@@ -2643,10 +2643,8 @@
 	 */
 	if (type == CCS_QUERY)
 		atomic_inc(&ccs_query_observers);
-	else if (type != CCS_AUDIT && type != CCS_VERSION &&
-		 type != CCS_MEMINFO && type != CCS_STAT)
-		head->reader_idx = ccs_lock();
 	file->private_data = head;
+	ccs_notify_gc(head, true);
 	return 0;
 }
 
@@ -2790,27 +2788,14 @@
 int ccs_close_control(struct file *file)
 {
 	struct ccs_io_buffer *head = file->private_data;
-	const bool is_write = head->write_buf != NULL;
-	const u8 type = head->type;
+	file->private_data = NULL;
 	/*
 	 * If the file is /proc/ccs/query, decrement the observer counter.
 	 */
-	if (type == CCS_QUERY) {
-		if (atomic_dec_and_test(&ccs_query_observers))
-			wake_up_all(&ccs_answer_wait);
-	} else if (type != CCS_AUDIT && type != CCS_VERSION &&
-		   type != CCS_MEMINFO && type != CCS_STAT)
-		ccs_unlock(head->reader_idx);
-	/* Release memory used for policy I/O. */
-	kfree(head->read_buf);
-	head->read_buf = NULL;
-	kfree(head->write_buf);
-	head->write_buf = NULL;
-	kfree(head);
-	head = NULL;
-	file->private_data = NULL;
-	if (is_write)
-		ccs_run_gc();
+	if (head->type == CCS_QUERY &&
+	    atomic_dec_and_test(&ccs_query_observers))
+		wake_up_all(&ccs_answer_wait);
+	ccs_notify_gc(head, false);
 	return 0;
 }
 
Index: security/ccsecurity/gc.c
===================================================================
--- security/ccsecurity/gc.c	(revision 4660)
+++ security/ccsecurity/gc.c	(working copy)
@@ -51,46 +51,97 @@
 
 #endif
 
+/* The list for "struct ccs_io_buffer". */
+static LIST_HEAD(ccs_io_buffer_list);
+/* Lock for protecting ccs_io_buffer_list. */
+static DEFINE_SPINLOCK(ccs_io_buffer_list_lock);
 
-/* Structure for garbage collection. */
-struct ccs_gc {
-	struct list_head list;
-	int type; /* One of values in "enum ccs_policy_id". */
-	struct list_head *element;
-};
-/* List of entries to be deleted. */
-static LIST_HEAD(ccs_gc_list);
-/* Length of ccs_gc_list. */
-static int ccs_gc_list_len;
-
 /**
- * ccs_add_to_gc - Add an entry to to be deleted list.
+ * ccs_struct_used_by_io_buffer - Check whether the list element is used by /proc/ccs/ users or not.
  *
- * @type:    Type of this entry.
  * @element: Pointer to "struct list_head".
  *
- * Returns true on success, false otherwise.
+ * Returns true if @element is used by /proc/ccs/ users, false otherwise.
+ */
+static bool ccs_struct_used_by_io_buffer(const struct list_head *element)
+{
+	struct ccs_io_buffer *head;
+	bool in_use = false;
+	//WARN_ONCE(1, "Entering %s\n", __func__);
+	spin_lock(&ccs_io_buffer_list_lock);
+	list_for_each_entry(head, &ccs_io_buffer_list, list) {
+		head->users++;
+		spin_unlock(&ccs_io_buffer_list_lock);
+		//WARN_ONCE(1, "Locking %s\n", __func__);
+		if (mutex_lock_interruptible(&head->io_sem)) {
+			in_use = true;
+			goto out;
+		}
+		if (WARN_ONCE(head->r.domain == element,
+			      "***** %p is used by r.domain *****\n", element))
+			in_use = true;
+		if (WARN_ONCE(head->r.group == element,
+			      "***** %p is used by r.group *****\n", element))
+			in_use = true;
+		if (WARN_ONCE(head->r.acl == element,
+			      "***** %p is used by r.acl *****\n", element))
+			in_use = true;
+		if (WARN_ONCE(&head->w.domain->list == element,
+			      "***** %p is used by w.domain->list *****\n",
+			      element))
+			in_use = true;
+		mutex_unlock(&head->io_sem);
+		//WARN_ONCE(1, "Unlocking %s\n", __func__);
+out:
+		spin_lock(&ccs_io_buffer_list_lock);
+		head->users--;
+		if (in_use)
+			break;
+	}
+	spin_unlock(&ccs_io_buffer_list_lock);
+	//WARN_ONCE(1, "Leaving %s\n", __func__);
+	return in_use;
+}
+
+/**
+ * ccs_name_used_by_io_buffer - Check whether the string is used by /proc/ccs/ users or not.
  *
- * Caller holds ccs_policy_lock mutex.
+ * @string: String to check.
+ * @size:   Memory allocated for @string .
  *
- * Adding an entry needs kmalloc(). Thus, if we try to add thousands of
- * entries at once, it will take too long time. Thus, do not add more than 128
- * entries per a scan. But to be able to handle worst case where all entries
- * are in-use, we accept one more entry per a scan.
- *
- * If we use singly linked list using "struct list_head"->prev (which is
- * LIST_POISON2), we can avoid kmalloc().
+ * Returns true if @string is used by /proc/ccs/ users, false otherwise.
  */
-static bool ccs_add_to_gc(const int type, struct list_head *element)
+static bool ccs_name_used_by_io_buffer(const char *string, const int size)
 {
-	struct ccs_gc *entry = kzalloc(sizeof(*entry), CCS_GFP_FLAGS);
-	if (!entry)
-		return false;
-	entry->type = type;
-	entry->element = element;
-	list_add(&entry->list, &ccs_gc_list);
-	list_del_rcu(element);
-	return ccs_gc_list_len++ < 128;
+	struct ccs_io_buffer *head;
+	bool in_use = false;
+	spin_lock(&ccs_io_buffer_list_lock);
+	list_for_each_entry(head, &ccs_io_buffer_list, list) {
+		int i;
+		head->users++;
+		spin_unlock(&ccs_io_buffer_list_lock);
+		if (mutex_lock_interruptible(&head->io_sem)) {
+			in_use = true;
+			goto out;
+		}
+		for (i = 0; i < CCS_MAX_IO_READ_QUEUE; i++) {
+			const char *w = head->r.w[i];
+			if (w < string || w > string + size)
+				continue;
+			WARN_ONCE(1, "***** %s is used by w[%u] *****\n",
+				  string, i);
+			in_use = true;
+			break;
+		}
+		mutex_unlock(&head->io_sem);
+out:
+		spin_lock(&ccs_io_buffer_list_lock);
+		head->users--;
+		if (in_use)
+			break;
+	}
+	spin_unlock(&ccs_io_buffer_list_lock);
+	return in_use;
 }
 
 /**
@@ -146,13 +197,13 @@
 #endif
 
 /**
- * ccs_used_by_task - Check whether the given pointer is referenced by a task.
+ * ccs_domain_used_by_task - Check whether the given pointer is referenced by a task.
  *
  * @domain: Pointer to "struct ccs_domain_info".
  *
  * Returns true if @domain is in use, false otherwise.
  */
-static bool ccs_used_by_task(struct ccs_domain_info *domain)
+static bool ccs_domain_used_by_task(struct ccs_domain_info *domain)
 {
 	bool in_use = false;
 	/*
@@ -377,7 +428,7 @@
 	 * By rechecking whether this domain is used by somebody or not at (8),
 	 * we can solve this race problem.
 	 */
-	if (ccs_used_by_task(domain))
+	if (ccs_domain_used_by_task(domain))
 		return 0;
 	for (i = 0; i < 2; i++) {
 		list_for_each_entry_safe(acl, tmp, &domain->acl_info_list[i],
@@ -540,24 +591,16 @@
 /*
  * Lock for syscall users.
  *
- * This lock is held for only protecting single SRCU section. Accessing
- * /proc/ccs/ interface cannot be finished within single SRCU section.
- * Therefore, we use ccs_lock()/ccs_unlock() for protecting /proc/ccs/ users.
- * Garbage collector waits for both this SRCU grace period and ccs_counter.
+ * This lock is held for only protecting single SRCU section.
  */
 struct srcu_struct ccs_ss;
 
-#endif
+#else
 
 /*
- * Lock for /proc/ccs/ users.
+ * Lock for syscall users.
  *
- * Holding SRCU lock upon open() and release upon close() causes lockdep to
- * complain about returning to userspace with SRCU lock held.
- * Therefore, non-SRCU lock is used in order to suppress the lockdep warning.
- * Modifying to hold/release SRCU lock upon each read()/write() is to-do list.
- *
- * This lock is also used for protecting single SRCU section for 2.6.18 and
+ * This lock is used for protecting single SRCU section for 2.6.18 and
  * earlier kernels because they don't have SRCU support.
  */
 static struct {
@@ -568,7 +611,7 @@
 static DEFINE_SPINLOCK(ccs_counter_lock);
 
 /**
- * ccs_lock - Hold non-SRCU lock.
+ * ccs_lock - Alternative for srcu_read_lock().
  *
  * Returns index number which has to be passed to ccs_unlock().
  */
@@ -583,7 +626,7 @@
 }
 
 /**
- * ccs_unlock - Release non-SRCU lock.
+ * ccs_unlock - Alternative for srcu_read_unlock().
  *
  * @idx: Index number returned by ccs_lock().
  *
@@ -597,7 +640,7 @@
 }
 
 /**
- * ccs_synchronize_counter - Wait for SRCU grace period.
+ * ccs_synchronize_counter - Alternative for synchronize_srcu().
  *
  * Returns nothing.
  */
@@ -617,10 +660,7 @@
 	ccs_counter.counter_idx ^= 1;
 	v = ccs_counter.counter[idx];
 	spin_unlock(&ccs_counter_lock);
-	/*
-	 * Waiting for /proc/ccs/ interface users to close() may take more than
-	 * a few seconds. Therefore, we should use ssleep() here.
-	 */
+	/* Wait for previously active counter to become 0. */
 	while (v) {
 		ssleep(1);
 		spin_lock(&ccs_counter_lock);
@@ -629,24 +669,121 @@
 	}
 }
 
+#endif
+
 /**
+ * ccs_delete_entry - Delete an entry.
+ *
+ * @id:      One of values in "enum ccs_policy_id".
+ * @element: Pointer to "struct list_head".
+ *
+ * Returns nothing.
+ */
+static void ccs_delete_entry(const enum ccs_policy_id id,
+			     struct list_head *element)
+{
+	size_t size;
+	if (ccs_struct_used_by_io_buffer(element))
+		return;
+	if (mutex_lock_interruptible(&ccs_policy_lock))
+		return;
+	/* Once remove from the list and wait for SRCU synchronization. */
+	//printk(KERN_INFO "***** Trying to remove %p\n", element);
+	list_del_rcu(element);
+	/*
+	 * We can't wait for SRCU synchronization without releasing
+	 * ccs_policy_lock because other threads take ccs_policy_lock within
+	 * SRCU read section.
+	 */
+	mutex_unlock(&ccs_policy_lock);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
+	synchronize_srcu(&ccs_ss);
+#else
+	ccs_synchronize_counter();
+#endif
+	/* Add back to the list if still in use after SRCU synchronization. */
+	if (ccs_struct_used_by_io_buffer(element))
+		goto add_again;
+	switch (id) {
+	case CCS_ID_TRANSITION_CONTROL:
+		size = ccs_del_transition_control(element);
+		break;
+	case CCS_ID_MANAGER:
+		size = ccs_del_manager(element);
+		break;
+	case CCS_ID_AGGREGATOR:
+		size = ccs_del_aggregator(element);
+		break;
+	case CCS_ID_GROUP:
+		size = ccs_del_group(element);
+		break;
+	case CCS_ID_PATH_GROUP:
+		size = ccs_del_path_group(element);
+		break;
+	case CCS_ID_ADDRESS_GROUP:
+		size = ccs_del_address_group(element);
+		break;
+	case CCS_ID_NUMBER_GROUP:
+		size = ccs_del_number_group(element);
+		break;
+	case CCS_ID_RESERVEDPORT:
+		size = ccs_del_reservedport(element);
+		break;
+	case CCS_ID_IPV6_ADDRESS:
+		size = ccs_del_ipv6_address(element);
+		break;
+	case CCS_ID_CONDITION:
+		size = ccs_del_condition(element);
+		break;
+	case CCS_ID_NAME:
+		size = ccs_del_name(element);
+		break;
+	case CCS_ID_ACL:
+		size = ccs_del_acl(element);
+		break;
+	case CCS_ID_DOMAIN:
+		size = ccs_del_domain(element);
+		if (!size)
+			goto add_again;
+		break;
+	default:
+		size = 0;
+		break;
+	}
+	//printk(KERN_INFO "***** Removing %p\n", element);
+	ccs_memory_free(element, size);
+	return;
+add_again:
+	/*
+	 * List elements may have been added while waiting for SRCU
+	 * synchronization. But since garbage collector thread is the only one
+	 * who removes list elements and the garbage collector thread is
+	 * exclusive, it is guaranteed that accessing element->next->prev
+	 * returns a valid pointer.
+	 */
+	mutex_lock(&ccs_policy_lock);
+	printk(KERN_INFO "***** Adding back %p\n", element);
+	__list_add_rcu(element, element->next->prev, element->next);
+	mutex_unlock(&ccs_policy_lock);
+}
+
+/**
  * ccs_collect_member - Delete elements with "struct ccs_acl_head".
  *
  * @member_list: Pointer to "struct list_head".
  * @id:          One of values in "enum ccs_policy_id".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
  */
-static bool ccs_collect_member(struct list_head *member_list, int id)
+static void ccs_collect_member(const enum ccs_policy_id id,
+			       struct list_head *member_list)
 {
 	struct ccs_acl_head *member;
-	list_for_each_entry(member, member_list, list) {
-		if (!member->is_deleted)
-			continue;
-		if (!ccs_add_to_gc(id, &member->list))
-			return false;
+	struct ccs_acl_head *tmp;
+	list_for_each_entry_safe(member, tmp, member_list, list) {
+		if (member->is_deleted)
+			ccs_delete_entry(id, &member->list);
 	}
-	return true;
 }
 
 /**
@@ -654,172 +791,23 @@
  *
  * @domain: Pointer to "struct ccs_domain_info".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
  */
-static bool ccs_collect_acl(struct ccs_domain_info *domain)
+static void ccs_collect_acl(struct ccs_domain_info *domain)
 {
 	struct ccs_acl_info *acl;
+	struct ccs_acl_info *tmp;
 	u8 i;
 	for (i = 0; i < 2; i++) {
-		list_for_each_entry(acl, &domain->acl_info_list[i], list) {
-			if (!acl->is_deleted)
-				continue;
-			if (!ccs_add_to_gc(CCS_ID_ACL, &acl->list))
-				return false;
+		list_for_each_entry_safe(acl, tmp, &domain->acl_info_list[i],
+					 list) {
+			if (acl->is_deleted)
+				ccs_delete_entry(CCS_ID_ACL, &acl->list);
 		}
 	}
-	return true;
 }
 
 /**
- * ccs_collect_entry - Scan lists for deleted elements.
- *
- * Returns nothing.
- */
-static void ccs_collect_entry(void)
-{
-	int i;
-	int idx;
-	if (mutex_lock_interruptible(&ccs_policy_lock))
-		return;
-	idx = ccs_read_lock();
-	for (i = 0; i < CCS_MAX_POLICY; i++)
-		if (!ccs_collect_member(&ccs_policy_list[i], i))
-			goto unlock;
-	for (i = 0; i < CCS_MAX_ACL_GROUPS; i++)
-		if (!ccs_collect_acl(&ccs_acl_group[i]))
-			goto unlock;
-	{
-		struct ccs_domain_info *domain;
-		list_for_each_entry(domain, &ccs_domain_list, list) {
-			if (!ccs_collect_acl(domain))
-				goto unlock;
-			if (!domain->is_deleted ||
-			    ccs_used_by_task(domain))
-				continue;
-			if (!ccs_add_to_gc(CCS_ID_DOMAIN, &domain->list))
-				goto unlock;
-		}
-	}
-	for (i = 0; i < CCS_MAX_GROUP; i++) {
-		struct list_head *list = &ccs_group_list[i];
-		int id;
-		struct ccs_group *group;
-		switch (i) {
-		case 0:
-			id = CCS_ID_PATH_GROUP;
-			break;
-		case 1:
-			id = CCS_ID_NUMBER_GROUP;
-			break;
-		default:
-			id = CCS_ID_ADDRESS_GROUP;
-			break;
-		}
-		list_for_each_entry(group, list, head.list) {
-			if (!ccs_collect_member(&group->member_list, id))
-				goto unlock;
-			if (!list_empty(&group->member_list) ||
-			    atomic_read(&group->head.users))
-				continue;
-			if (!ccs_add_to_gc(CCS_ID_GROUP, &group->head.list))
-				goto unlock;
-		}
-	}
-	for (i = 0; i < CCS_MAX_LIST + CCS_MAX_HASH; i++) {
-		struct list_head *list = i < CCS_MAX_LIST ?
-			&ccs_shared_list[i] : &ccs_name_list[i - CCS_MAX_LIST];
-		int id;
-		struct ccs_shared_acl_head *ptr;
-		switch (i) {
-		case 0:
-			id = CCS_ID_CONDITION;
-			break;
-		case 1:
-			id = CCS_ID_IPV6_ADDRESS;
-			break;
-		default:
-			id = CCS_ID_NAME;
-			break;
-		}
-		list_for_each_entry(ptr, list, list) {
-			if (atomic_read(&ptr->users))
-				continue;
-			if (!ccs_add_to_gc(id, &ptr->list))
-				goto unlock;
-		}
-	}
-unlock:
-	ccs_read_unlock(idx);
-	mutex_unlock(&ccs_policy_lock);
-}
-
-/**
- * ccs_kfree_entry - Delete entries in ccs_gc_list.
- *
- * Returns true if some entries were kfree()d, false otherwise.
- */
-static bool ccs_kfree_entry(void)
-{
-	struct ccs_gc *p;
-	struct ccs_gc *tmp;
-	bool result = false;
-	list_for_each_entry_safe(p, tmp, &ccs_gc_list, list) {
-		size_t size = 0;
-		struct list_head * const element = p->element;
-		switch (p->type) {
-		case CCS_ID_TRANSITION_CONTROL:
-			size = ccs_del_transition_control(element);
-			break;
-		case CCS_ID_MANAGER:
-			size = ccs_del_manager(element);
-			break;
-		case CCS_ID_AGGREGATOR:
-			size = ccs_del_aggregator(element);
-			break;
-		case CCS_ID_GROUP:
-			size = ccs_del_group(element);
-			break;
-		case CCS_ID_PATH_GROUP:
-			size = ccs_del_path_group(element);
-			break;
-		case CCS_ID_ADDRESS_GROUP:
-			size = ccs_del_address_group(element);
-			break;
-		case CCS_ID_NUMBER_GROUP:
-			size = ccs_del_number_group(element);
-			break;
-		case CCS_ID_RESERVEDPORT:
-			size = ccs_del_reservedport(element);
-			break;
-		case CCS_ID_IPV6_ADDRESS:
-			size = ccs_del_ipv6_address(element);
-			break;
-		case CCS_ID_CONDITION:
-			size = ccs_del_condition(element);
-			break;
-		case CCS_ID_NAME:
-			size = ccs_del_name(element);
-			break;
-		case CCS_ID_ACL:
-			size = ccs_del_acl(element);
-			break;
-		case CCS_ID_DOMAIN:
-			size = ccs_del_domain(element);
-			if (!size)
-				continue;
-			break;
-		}
-		ccs_memory_free(element, size);
-		list_del(&p->list);
-		kfree(p);
-		ccs_gc_list_len--;
-		result = true;
-	}
-	return result;
-}
-
-/**
  * ccs_gc_thread - Garbage collector thread function.
  *
  * @unused: Unused.
@@ -831,6 +819,8 @@
  */
 static int ccs_gc_thread(void *unused)
 {
+	int i;
+	enum ccs_policy_id id;
 	/* Garbage collector thread is exclusive. */
 	static DEFINE_MUTEX(ccs_gc_mutex);
 	if (!mutex_trylock(&ccs_gc_mutex))
@@ -859,15 +849,84 @@
 #endif
 	snprintf(current->comm, sizeof(current->comm) - 1, "GC for CCS");
 #endif
-	do {
-		ccs_collect_entry();
-		if (list_empty(&ccs_gc_list))
+	for (id = 0; id < CCS_MAX_POLICY; id++)
+		ccs_collect_member(id, &ccs_policy_list[id]);
+	for (i = 0; i < CCS_MAX_ACL_GROUPS; i++)
+		ccs_collect_acl(&ccs_acl_group[i]);
+	{
+		struct ccs_domain_info *domain;
+		struct ccs_domain_info *tmp;
+		list_for_each_entry_safe(domain, tmp, &ccs_domain_list, list) {
+			ccs_collect_acl(domain);
+			if (!domain->is_deleted ||
+			    ccs_domain_used_by_task(domain))
+				continue;
+			ccs_delete_entry(CCS_ID_DOMAIN, &domain->list);
+		}
+	}
+	for (i = 0; i < CCS_MAX_GROUP; i++) {
+		struct list_head *list = &ccs_group_list[i];
+		struct ccs_group *group;
+		struct ccs_group *tmp;
+		switch (i) {
+		case 0:
+			id = CCS_ID_PATH_GROUP;
 			break;
-		ccs_synchronize_counter();
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
-		synchronize_srcu(&ccs_ss);
-#endif
-	} while (ccs_kfree_entry());
+		case 1:
+			id = CCS_ID_NUMBER_GROUP;
+			break;
+		default:
+			id = CCS_ID_ADDRESS_GROUP;
+			break;
+		}
+		list_for_each_entry_safe(group, tmp, list, head.list) {
+			ccs_collect_member(id, &group->member_list);
+			if (!list_empty(&group->member_list) ||
+			    atomic_read(&group->head.users))
+				continue;
+			ccs_delete_entry(CCS_ID_GROUP, &group->head.list);
+		}
+	}
+	for (i = 0; i < CCS_MAX_LIST; i++) {
+		struct list_head *list = &ccs_shared_list[i];
+		enum ccs_policy_id id = !i ?
+			CCS_ID_CONDITION : CCS_ID_IPV6_ADDRESS;
+		struct ccs_shared_acl_head *ptr;
+		struct ccs_shared_acl_head *tmp;
+		list_for_each_entry_safe(ptr, tmp, list, list) {
+			if (!atomic_read(&ptr->users))
+				ccs_delete_entry(id, &ptr->list);
+		}
+	}
+	for (i = 0; i < CCS_MAX_HASH; i++) {
+		struct list_head *list = &ccs_name_list[i];
+		struct ccs_shared_acl_head *ptr;
+		struct ccs_shared_acl_head *tmp;
+		list_for_each_entry_safe(ptr, tmp, list, list) {
+			struct ccs_name *p =
+				container_of(ptr, typeof(*p), head);
+			if (atomic_read(&ptr->users) ||
+			    ccs_name_used_by_io_buffer(p->entry.name, p->size))
+				continue;
+			ccs_delete_entry(CCS_ID_NAME, &ptr->list);
+		}
+	}
+	{
+		struct ccs_io_buffer *head;
+		struct ccs_io_buffer *tmp;
+		spin_lock(&ccs_io_buffer_list_lock);
+		list_for_each_entry_safe(head, tmp, &ccs_io_buffer_list,
+					 list) {
+			if (head->users)
+				continue;
+			//printk(KERN_INFO "***** Releasing %p\n", head);
+			list_del(&head->list);
+			kfree(head->read_buf);
+			kfree(head->write_buf);
+			kfree(head);
+		}
+		spin_unlock(&ccs_io_buffer_list_lock);
+	}
 	mutex_unlock(&ccs_gc_mutex);
 out:
 	/* This acts as do_exit(0). */
@@ -875,18 +934,37 @@
 }
 
 /**
- * ccs_run_gc - Start garbage collector thread.
+ * ccs_notify_gc - Register/unregister /proc/ccs/ users.
  *
+ * @head:        Pointer to "struct ccs_io_buffer".
+ * @is_register: True if register, false if unregister.
+ *
  * Returns nothing.
  */
-void ccs_run_gc(void)
+void ccs_notify_gc(struct ccs_io_buffer *head, const bool is_register)
 {
+	spin_lock(&ccs_io_buffer_list_lock);
+	if (is_register) {
+		head->users = 1;
+		list_add(&head->list, &ccs_io_buffer_list);
+	} else {
+		if (!--head->users) {
+			//printk(KERN_INFO "***** Releasing %p\n", head);
+			list_del(&head->list);
+			kfree(head->read_buf);
+			kfree(head->write_buf);
+			kfree(head);
+		}
+	}
+	spin_unlock(&ccs_io_buffer_list_lock);
+	if (!is_register) {
 #if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 6)
-	struct task_struct *task = kthread_create(ccs_gc_thread, NULL,
-						  "GC for CCS");
-	if (!IS_ERR(task))
-		wake_up_process(task);
+		struct task_struct *task = kthread_create(ccs_gc_thread, NULL,
+							  "GC for CCS");
+		if (!IS_ERR(task))
+			wake_up_process(task);
 #else
-	kernel_thread(ccs_gc_thread, NULL, 0);
+		kernel_thread(ccs_gc_thread, NULL, 0);
 #endif
+	}
 }
