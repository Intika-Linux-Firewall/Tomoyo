Index: security/ccsecurity/internal.h
===================================================================
--- security/ccsecurity/internal.h	(revision 4649)
+++ security/ccsecurity/internal.h	(working copy)
@@ -1314,15 +1314,13 @@
 	bool is_delete;
 };
 
-/* Structure for reading/writing policy via /proc interfaces. */
+/* Structure for reading/writing policy via /proc/ccs/ interfaces. */
 struct ccs_io_buffer {
 	void (*read) (struct ccs_io_buffer *);
 	int (*write) (struct ccs_io_buffer *);
 	int (*poll) (struct file *file, poll_table *wait);
 	/* Exclusive lock for this structure.   */
 	struct mutex io_sem;
-	/* Index returned by ccs_lock().        */
-	int reader_idx;
 	char __user *read_user_buf;
 	int read_user_buf_avail;
 	struct {
@@ -1358,6 +1356,8 @@
 	int writebuf_size;
 	/* One of values in "enum ccs_proc_interface_index". */
 	u8 type;
+	/* List for telling GC not to kfree() elements. */
+	struct list_head list;
 };
 
 /* Structure for /proc/ccs/profile interface. */
@@ -1434,7 +1434,6 @@
 int ccs_env_perm(struct ccs_request_info *r, const char *env);
 int ccs_get_path(const char *pathname, struct path *path);
 int ccs_init_request_info(struct ccs_request_info *r, const u8 index);
-int ccs_lock(void);
 int ccs_open_control(const u8 type, struct file *file);
 int ccs_parse_ip_address(char *address, u16 *min, u16 *max);
 int ccs_path_permission(struct ccs_request_info *r, u8 operation,
@@ -1489,6 +1488,7 @@
 void ccs_get_attributes(struct ccs_obj_info *obj);
 void ccs_memory_free(const void *ptr, size_t size);
 void ccs_normalize_line(unsigned char *buffer);
+void ccs_notify_gc(struct ccs_io_buffer *head, const bool is_register);
 void ccs_print_ipv4(char *buffer, const int buffer_len, const u32 min_ip,
 		    const u32 max_ip);
 void ccs_print_ipv6(char *buffer, const int buffer_len,
@@ -1501,7 +1501,6 @@
 void ccs_read_log(struct ccs_io_buffer *head);
 void ccs_run_gc(void);
 void ccs_transition_failed(const char *domainname);
-void ccs_unlock(const int idx);
 void ccs_update_stat(const u8 index);
 void ccs_warn_oom(const char *function);
 void ccs_write_log(struct ccs_request_info *r, const char *fmt, ...)
@@ -1613,6 +1612,9 @@
 
 #else
 
+int ccs_lock(void);
+void ccs_unlock(const int idx);
+
 /**
  * ccs_read_lock - Take lock for protecting policy.
  *
Index: security/ccsecurity/policy_io.c
===================================================================
--- security/ccsecurity/policy_io.c	(revision 4649)
+++ security/ccsecurity/policy_io.c	(working copy)
@@ -2643,10 +2643,8 @@
 	 */
 	if (type == CCS_QUERY)
 		atomic_inc(&ccs_query_observers);
-	else if (type != CCS_AUDIT && type != CCS_VERSION &&
-		 type != CCS_MEMINFO && type != CCS_STAT)
-		head->reader_idx = ccs_lock();
 	file->private_data = head;
+	ccs_notify_gc(head, true);
 	return 0;
 }
 
@@ -2795,12 +2793,10 @@
 	/*
 	 * If the file is /proc/ccs/query, decrement the observer counter.
 	 */
-	if (type == CCS_QUERY) {
-		if (atomic_dec_and_test(&ccs_query_observers))
-			wake_up_all(&ccs_answer_wait);
-	} else if (type != CCS_AUDIT && type != CCS_VERSION &&
-		   type != CCS_MEMINFO && type != CCS_STAT)
-		ccs_unlock(head->reader_idx);
+	if (type == CCS_QUERY &&
+	    atomic_dec_and_test(&ccs_query_observers))
+		wake_up_all(&ccs_answer_wait);
+	ccs_notify_gc(head, false);
 	/* Release memory used for policy I/O. */
 	kfree(head->read_buf);
 	head->read_buf = NULL;
Index: security/ccsecurity/gc.c
===================================================================
--- security/ccsecurity/gc.c	(revision 4649)
+++ security/ccsecurity/gc.c	(working copy)
@@ -51,46 +51,63 @@
 
 #endif
 
-
-/* Structure for garbage collection. */
-struct ccs_gc {
-	struct list_head list;
-	int type; /* One of values in "enum ccs_policy_id". */
-	struct list_head *element;
-};
-/* List of entries to be deleted. */
+/* The list for "struct ccs_io_buffer". */
 static LIST_HEAD(ccs_gc_list);
-/* Length of ccs_gc_list. */
-static int ccs_gc_list_len;
+/* Lock for protecting ccs_gc_list. */
+static DEFINE_SPINLOCK(ccs_gc_lock);
 
 /**
- * ccs_add_to_gc - Add an entry to to be deleted list.
+ * ccs_notify_gc - Register/unregister /proc/ccs/ users.
  *
- * @type:    Type of this entry.
- * @element: Pointer to "struct list_head".
+ * @head:        Pointer to "struct ccs_io_buffer".
+ * @is_register: True if register, false if unregister.
  *
- * Returns true on success, false otherwise.
+ * Returns nothing.
+ */
+void ccs_notify_gc(struct ccs_io_buffer *head, const bool is_register)
+{
+	spin_lock(&ccs_gc_lock);
+	if (is_register)
+		list_add(&head->list, &ccs_gc_list);
+	else
+		list_del(&head->list);
+	spin_unlock(&ccs_gc_lock);
+}
+
+/**
+ * ccs_used_by_io_buffer - Check whether the list element is used by /proc/ccs/ users or not.
  *
- * Caller holds ccs_policy_lock mutex.
+ * @element: Pointer to "struct list_head".
  *
- * Adding an entry needs kmalloc(). Thus, if we try to add thousands of
- * entries at once, it will take too long time. Thus, do not add more than 128
- * entries per a scan. But to be able to handle worst case where all entries
- * are in-use, we accept one more entry per a scan.
- *
- * If we use singly linked list using "struct list_head"->prev (which is
- * LIST_POISON2), we can avoid kmalloc().
+ * Returns true if @element is used by /proc/ccs/ users, false otherwise.
  */
-static bool ccs_add_to_gc(const int type, struct list_head *element)
+static bool ccs_used_by_io_buffer(const struct list_head *element)
 {
-	struct ccs_gc *entry = kzalloc(sizeof(*entry), CCS_GFP_FLAGS);
-	if (!entry)
-		return false;
-	entry->type = type;
-	entry->element = element;
-	list_add(&entry->list, &ccs_gc_list);
-	list_del_rcu(element);
-	return ccs_gc_list_len++ < 128;
+	struct ccs_io_buffer *head;
+	bool in_use = false;
+	spin_lock(&ccs_gc_lock);
+	list_for_each_entry(head, &ccs_gc_list, list) {
+		if (head->r.domain == element)
+			printk(KERN_INFO
+			       "***** %p is used by r.domain *****\n",
+			       element);
+		else if (head->r.group == element)
+			printk(KERN_INFO
+			       "***** %p is used by r.group *****\n", element);
+		else if (head->r.acl == element)
+			printk(KERN_INFO
+			       "***** %p is used by r.acl *****\n", element);
+		else if (&head->w.domain->list == element)
+			printk(KERN_INFO
+			       "***** %p is used by w.domain->list *****\n",
+			       element);
+		else
+			continue;
+		in_use = true;
+		break;
+	}
+	spin_unlock(&ccs_gc_lock);
+	return in_use;
 }
 
 /**
@@ -540,24 +557,16 @@
 /*
  * Lock for syscall users.
  *
- * This lock is held for only protecting single SRCU section. Accessing
- * /proc/ccs/ interface cannot be finished within single SRCU section.
- * Therefore, we use ccs_lock()/ccs_unlock() for protecting /proc/ccs/ users.
- * Garbage collector waits for both this SRCU grace period and ccs_counter.
+ * This lock is held for only protecting single SRCU section.
  */
 struct srcu_struct ccs_ss;
 
-#endif
+#else
 
 /*
- * Lock for /proc/ccs/ users.
+ * Lock for syscall users.
  *
- * Holding SRCU lock upon open() and release upon close() causes lockdep to
- * complain about returning to userspace with SRCU lock held.
- * Therefore, non-SRCU lock is used in order to suppress the lockdep warning.
- * Modifying to hold/release SRCU lock upon each read()/write() is to-do list.
- *
- * This lock is also used for protecting single SRCU section for 2.6.18 and
+ * This lock is used for protecting single SRCU section for 2.6.18 and
  * earlier kernels because they don't have SRCU support.
  */
 static struct {
@@ -568,7 +577,7 @@
 static DEFINE_SPINLOCK(ccs_counter_lock);
 
 /**
- * ccs_lock - Hold non-SRCU lock.
+ * ccs_lock - Alternative for srcu_read_lock().
  *
  * Returns index number which has to be passed to ccs_unlock().
  */
@@ -583,7 +592,7 @@
 }
 
 /**
- * ccs_unlock - Release non-SRCU lock.
+ * ccs_unlock - Alternative for srcu_read_unlock().
  *
  * @idx: Index number returned by ccs_lock().
  *
@@ -597,7 +606,7 @@
 }
 
 /**
- * ccs_synchronize_counter - Wait for SRCU grace period.
+ * ccs_synchronize_counter - Alternative for synchronize_srcu().
  *
  * Returns nothing.
  */
@@ -617,10 +626,7 @@
 	ccs_counter.counter_idx ^= 1;
 	v = ccs_counter.counter[idx];
 	spin_unlock(&ccs_counter_lock);
-	/*
-	 * Waiting for /proc/ccs/ interface users to close() may take more than
-	 * a few seconds. Therefore, we should use ssleep() here.
-	 */
+	/* Wait for previously active counter to become 0. */
 	while (v) {
 		ssleep(1);
 		spin_lock(&ccs_counter_lock);
@@ -629,24 +635,103 @@
 	}
 }
 
+#endif
+
 /**
+ * ccs_delete_entry - Delete an entry.
+ *
+ * @id:      One of values in "enum ccs_policy_id".
+ * @element: Pointer to "struct list_head".
+ *
+ * Returns nnothing.
+ *
+ * Caller holds ccs_policy_lock mutex.
+ */
+static void ccs_delete_entry(const enum ccs_policy_id id,
+			     struct list_head *element)
+{
+	struct list_head *prev = element->prev;
+	size_t size;
+	if (ccs_used_by_io_buffer(element))
+		return;
+	list_del_rcu(element);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
+	synchronize_srcu(&ccs_ss);
+#else
+	ccs_synchronize_counter();
+#endif
+	if (ccs_used_by_io_buffer(element))
+		goto add_again;
+	switch (id) {
+	case CCS_ID_TRANSITION_CONTROL:
+		size = ccs_del_transition_control(element);
+		break;
+	case CCS_ID_MANAGER:
+		size = ccs_del_manager(element);
+		break;
+	case CCS_ID_AGGREGATOR:
+		size = ccs_del_aggregator(element);
+		break;
+	case CCS_ID_GROUP:
+		size = ccs_del_group(element);
+		break;
+	case CCS_ID_PATH_GROUP:
+		size = ccs_del_path_group(element);
+		break;
+	case CCS_ID_ADDRESS_GROUP:
+		size = ccs_del_address_group(element);
+		break;
+	case CCS_ID_NUMBER_GROUP:
+		size = ccs_del_number_group(element);
+		break;
+	case CCS_ID_RESERVEDPORT:
+		size = ccs_del_reservedport(element);
+		break;
+	case CCS_ID_IPV6_ADDRESS:
+		size = ccs_del_ipv6_address(element);
+		break;
+	case CCS_ID_CONDITION:
+		size = ccs_del_condition(element);
+		break;
+	case CCS_ID_NAME:
+		size = ccs_del_name(element);
+		break;
+	case CCS_ID_ACL:
+		size = ccs_del_acl(element);
+		break;
+	case CCS_ID_DOMAIN:
+		size = ccs_del_domain(element);
+		if (!size)
+			goto add_again;
+		break;
+	default:
+		size = 0;
+		break;
+	}
+	ccs_memory_free(element, size);
+	return;
+add_again:
+	element->prev = prev;
+	prev->next = element;
+}
+
+/**
  * ccs_collect_member - Delete elements with "struct ccs_acl_head".
  *
  * @member_list: Pointer to "struct list_head".
  * @id:          One of values in "enum ccs_policy_id".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
  */
-static bool ccs_collect_member(struct list_head *member_list, int id)
+static void ccs_collect_member(const enum ccs_policy_id id,
+			       struct list_head *member_list)
 {
 	struct ccs_acl_head *member;
-	list_for_each_entry(member, member_list, list) {
-		if (!member->is_deleted)
-			continue;
-		if (!ccs_add_to_gc(id, &member->list))
-			return false;
+	struct ccs_acl_head *tmp;
+	list_for_each_entry_safe(member, tmp, member_list, list) {
+		if (member->is_deleted)
+			ccs_delete_entry(id, &member->list);
 	}
-	return true;
 }
 
 /**
@@ -654,57 +739,85 @@
  *
  * @domain: Pointer to "struct ccs_domain_info".
  *
- * Returns true if some elements are deleted, false otherwise.
+ * Returns nothing.
  */
-static bool ccs_collect_acl(struct ccs_domain_info *domain)
+static void ccs_collect_acl(struct ccs_domain_info *domain)
 {
 	struct ccs_acl_info *acl;
+	struct ccs_acl_info *tmp;
 	u8 i;
 	for (i = 0; i < 2; i++) {
-		list_for_each_entry(acl, &domain->acl_info_list[i], list) {
-			if (!acl->is_deleted)
-				continue;
-			if (!ccs_add_to_gc(CCS_ID_ACL, &acl->list))
-				return false;
+		list_for_each_entry_safe(acl, tmp, &domain->acl_info_list[i],
+					 list) {
+			if (acl->is_deleted)
+				ccs_delete_entry(CCS_ID_ACL, &acl->list);
 		}
 	}
-	return true;
 }
 
 /**
- * ccs_collect_entry - Scan lists for deleted elements.
+ * ccs_gc_thread - Garbage collector thread function.
  *
- * Returns nothing.
+ * @unused: Unused.
+ *
+ * In case OOM-killer choose this thread for termination, we create this thread
+ * as a short live thread whenever /proc/ccs/ interface was close()d.
+ *
+ * Returns 0.
  */
-static void ccs_collect_entry(void)
+static int ccs_gc_thread(void *unused)
 {
 	int i;
-	int idx;
+	enum ccs_policy_id id;
+	/* Garbage collector thread is exclusive. */
+	static DEFINE_MUTEX(ccs_gc_mutex);
+	if (!mutex_trylock(&ccs_gc_mutex))
+		goto out;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
+	daemonize("GC for CCS");
+#else
+	daemonize();
+	reparent_to_init();
+#if defined(TASK_DEAD)
+	{
+		struct task_struct *task = current;
+		spin_lock_irq(&task->sighand->siglock);
+		siginitsetinv(&task->blocked, 0);
+		recalc_sigpending();
+		spin_unlock_irq(&task->sighand->siglock);
+	}
+#else
+	{
+		struct task_struct *task = current;
+		spin_lock_irq(&task->sigmask_lock);
+		siginitsetinv(&task->blocked, 0);
+		recalc_sigpending(task);
+		spin_unlock_irq(&task->sigmask_lock);
+	}
+#endif
+	snprintf(current->comm, sizeof(current->comm) - 1, "GC for CCS");
+#endif
 	if (mutex_lock_interruptible(&ccs_policy_lock))
-		return;
-	idx = ccs_read_lock();
-	for (i = 0; i < CCS_MAX_POLICY; i++)
-		if (!ccs_collect_member(&ccs_policy_list[i], i))
-			goto unlock;
+		goto out_unlock;
+	for (id = 0; id < CCS_MAX_POLICY; id++)
+		ccs_collect_member(id, &ccs_policy_list[id]);
 	for (i = 0; i < CCS_MAX_ACL_GROUPS; i++)
-		if (!ccs_collect_acl(&ccs_acl_group[i]))
-			goto unlock;
+		ccs_collect_acl(&ccs_acl_group[i]);
 	{
 		struct ccs_domain_info *domain;
-		list_for_each_entry(domain, &ccs_domain_list, list) {
-			if (!ccs_collect_acl(domain))
-				goto unlock;
+		struct ccs_domain_info *tmp;
+		list_for_each_entry_safe(domain, tmp, &ccs_domain_list, list) {
+			ccs_collect_acl(domain);
 			if (!domain->is_deleted ||
 			    ccs_used_by_task(domain))
 				continue;
-			if (!ccs_add_to_gc(CCS_ID_DOMAIN, &domain->list))
-				goto unlock;
+			ccs_delete_entry(CCS_ID_DOMAIN, &domain->list);
 		}
 	}
 	for (i = 0; i < CCS_MAX_GROUP; i++) {
 		struct list_head *list = &ccs_group_list[i];
-		int id;
 		struct ccs_group *group;
+		struct ccs_group *tmp;
 		switch (i) {
 		case 0:
 			id = CCS_ID_PATH_GROUP;
@@ -716,21 +829,20 @@
 			id = CCS_ID_ADDRESS_GROUP;
 			break;
 		}
-		list_for_each_entry(group, list, head.list) {
-			if (!ccs_collect_member(&group->member_list, id))
-				goto unlock;
+		list_for_each_entry_safe(group, tmp, list, head.list) {
+			ccs_collect_member(id, &group->member_list);
 			if (!list_empty(&group->member_list) ||
 			    atomic_read(&group->head.users))
 				continue;
-			if (!ccs_add_to_gc(CCS_ID_GROUP, &group->head.list))
-				goto unlock;
+			ccs_delete_entry(CCS_ID_GROUP, &group->head.list);
 		}
 	}
 	for (i = 0; i < CCS_MAX_LIST + CCS_MAX_HASH; i++) {
 		struct list_head *list = i < CCS_MAX_LIST ?
 			&ccs_shared_list[i] : &ccs_name_list[i - CCS_MAX_LIST];
-		int id;
+		enum ccs_policy_id id;
 		struct ccs_shared_acl_head *ptr;
+		struct ccs_shared_acl_head *tmp;
 		switch (i) {
 		case 0:
 			id = CCS_ID_CONDITION;
@@ -742,132 +854,13 @@
 			id = CCS_ID_NAME;
 			break;
 		}
-		list_for_each_entry(ptr, list, list) {
-			if (atomic_read(&ptr->users))
-				continue;
-			if (!ccs_add_to_gc(id, &ptr->list))
-				goto unlock;
+		list_for_each_entry_safe(ptr, tmp, list, list) {
+			if (!atomic_read(&ptr->users))
+				ccs_delete_entry(id, &ptr->list);
 		}
 	}
-unlock:
-	ccs_read_unlock(idx);
 	mutex_unlock(&ccs_policy_lock);
-}
-
-/**
- * ccs_kfree_entry - Delete entries in ccs_gc_list.
- *
- * Returns true if some entries were kfree()d, false otherwise.
- */
-static bool ccs_kfree_entry(void)
-{
-	struct ccs_gc *p;
-	struct ccs_gc *tmp;
-	bool result = false;
-	list_for_each_entry_safe(p, tmp, &ccs_gc_list, list) {
-		size_t size = 0;
-		struct list_head * const element = p->element;
-		switch (p->type) {
-		case CCS_ID_TRANSITION_CONTROL:
-			size = ccs_del_transition_control(element);
-			break;
-		case CCS_ID_MANAGER:
-			size = ccs_del_manager(element);
-			break;
-		case CCS_ID_AGGREGATOR:
-			size = ccs_del_aggregator(element);
-			break;
-		case CCS_ID_GROUP:
-			size = ccs_del_group(element);
-			break;
-		case CCS_ID_PATH_GROUP:
-			size = ccs_del_path_group(element);
-			break;
-		case CCS_ID_ADDRESS_GROUP:
-			size = ccs_del_address_group(element);
-			break;
-		case CCS_ID_NUMBER_GROUP:
-			size = ccs_del_number_group(element);
-			break;
-		case CCS_ID_RESERVEDPORT:
-			size = ccs_del_reservedport(element);
-			break;
-		case CCS_ID_IPV6_ADDRESS:
-			size = ccs_del_ipv6_address(element);
-			break;
-		case CCS_ID_CONDITION:
-			size = ccs_del_condition(element);
-			break;
-		case CCS_ID_NAME:
-			size = ccs_del_name(element);
-			break;
-		case CCS_ID_ACL:
-			size = ccs_del_acl(element);
-			break;
-		case CCS_ID_DOMAIN:
-			size = ccs_del_domain(element);
-			if (!size)
-				continue;
-			break;
-		}
-		ccs_memory_free(element, size);
-		list_del(&p->list);
-		kfree(p);
-		ccs_gc_list_len--;
-		result = true;
-	}
-	return result;
-}
-
-/**
- * ccs_gc_thread - Garbage collector thread function.
- *
- * @unused: Unused.
- *
- * In case OOM-killer choose this thread for termination, we create this thread
- * as a short live thread whenever /proc/ccs/ interface was close()d.
- *
- * Returns 0.
- */
-static int ccs_gc_thread(void *unused)
-{
-	/* Garbage collector thread is exclusive. */
-	static DEFINE_MUTEX(ccs_gc_mutex);
-	if (!mutex_trylock(&ccs_gc_mutex))
-		goto out;
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 0)
-	daemonize("GC for CCS");
-#else
-	daemonize();
-	reparent_to_init();
-#if defined(TASK_DEAD)
-	{
-		struct task_struct *task = current;
-		spin_lock_irq(&task->sighand->siglock);
-		siginitsetinv(&task->blocked, 0);
-		recalc_sigpending();
-		spin_unlock_irq(&task->sighand->siglock);
-	}
-#else
-	{
-		struct task_struct *task = current;
-		spin_lock_irq(&task->sigmask_lock);
-		siginitsetinv(&task->blocked, 0);
-		recalc_sigpending(task);
-		spin_unlock_irq(&task->sigmask_lock);
-	}
-#endif
-	snprintf(current->comm, sizeof(current->comm) - 1, "GC for CCS");
-#endif
-	do {
-		ccs_collect_entry();
-		if (list_empty(&ccs_gc_list))
-			break;
-		ccs_synchronize_counter();
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 19)
-		synchronize_srcu(&ccs_ss);
-#endif
-	} while (ccs_kfree_entry());
+out_unlock:
 	mutex_unlock(&ccs_gc_mutex);
 out:
 	/* This acts as do_exit(0). */
